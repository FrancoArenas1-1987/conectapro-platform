PROMPT MAESTRO â€” PROYECTO CONECTAPRO (v2 Â· ORIENTADO A PRODUCCIÃ“N)
1. Contexto general

EstÃ¡s trabajando sobre el proyecto ConectaPro, una plataforma productiva, multi-servicio y multi-proveedor, cuyo objetivo es conectar clientes reales con tÃ©cnicos reales, a travÃ©s de WhatsApp, con foco en:

operaciÃ³n confiable

trazabilidad completa

control de calidad

escalabilidad comercial

ConectaPro NO es un chatbot.
Es un orquestador de servicios transaccional, con reglas claras y comportamiento predecible.

El objetivo final es salir al mercado con un producto funcional, no solo validar tecnologÃ­a.

2. Fuente de verdad y control de cambios (NO romper)

El repositorio se entrega siempre como un ZIP

Ese ZIP es la fuente de verdad absoluta del cÃ³digo

No se inventan archivos que no existan

No se cambian estructuras sin justificaciÃ³n tÃ©cnica fuerte

Los cambios deben ser:

mÃ­nimos

coherentes

compatibles hacia atrÃ¡s

auditables

ğŸ‘‰ La estabilidad es prioritaria sobre la velocidad.

3. Principio rector del sistema (CRÃTICO)

El sistema es determinÃ­stico y auditable.

Existe explÃ­citamente:

ConversationState

Lead.status

reglas de transiciÃ³n claras

El LLM:

âŒ NO decide flujos

âŒ NO asigna proveedores

âŒ NO cambia estados

âŒ NO escribe en la base de datos

âŒ NO â€œinterpretaâ€ reglas de negocio

El backend:

âœ… decide

âœ… valida

âœ… persiste

âœ… controla errores

âœ… garantiza consistencia

4. Uso de OpenAI (con guardrails estrictos)

OpenAI se usa solo como capa auxiliar, nunca como fuente de verdad.

OpenAI se utiliza exclusivamente para:

Comprender lenguaje natural libre

Extraer intenciÃ³n y campos estructurados

Reformular respuestas para tono humano

Manejar desvÃ­os conversacionales (â€œme salÃ­ del flujoâ€)

Reglas duras:

Si OpenAI falla â†’ fallback inmediato al flujo determinÃ­stico

El MVP nunca puede caerse por depender del LLM

El LLM debe operar bajo feature flag

El sistema debe poder operar sin OpenAI habilitado

5. Objetivo actual del desarrollo (FOCO PRODUCTIVO)

El objetivo actual es evolucionar el MVP hacia producto productivo, manteniendo:

Webhook WhatsApp existente

leads_flow.py como cerebro central

DB y modelos actuales

Matching determinÃ­stico

Worker de cierre automÃ¡tico

La integraciÃ³n con OpenAI debe:

mejorar UX conversacional

permitir texto libre

no comprometer control ni trazabilidad

no introducir estados implÃ­citos

6. Arquitectura obligatoria para OpenAI

OpenAI NO se mezcla con lÃ³gica de negocio.

Debe existir una capa aislada y versionable:

services/api/llm/
 â”œâ”€â”€ llm_client.py        # cliente OpenAI
 â”œâ”€â”€ intent_parser.py    # extracciÃ³n estructurada
 â”œâ”€â”€ prompt_templates.py # prompts versionados


ğŸ‘‰ Esta capa NO importa modelos ni DB.

7. Flujo obligatorio del mensaje
Mensaje usuario
   â†“
OpenAI â†’ JSON estructurado (intent, campos, confidence)
   â†“
ValidaciÃ³n backend (allowlists, confidence, estado)
   â†“
State Machine (decisiÃ³n determinÃ­stica)
   â†“
Respuesta:
   - determinÃ­stica
   - opcionalmente reescrita por LLM

8. Esquema estÃ¡ndar de intenciÃ³n (INQUEBRANTABLE)

El LLM debe devolver EXCLUSIVAMENTE JSON, sin texto adicional:

{
  "intent": "create_lead | update_lead | ask_status | cancel | smalltalk | unknown",
  "service": "Electricidad | Gasfiteria | Cerrajeria | null",
  "comuna": "string | null",
  "problem_type": "string | null",
  "urgency": "hoy | 1_2_dias | semana | null",
  "address": "string | null",
  "consent": "yes | no | null",
  "confidence": 0.0
}

Reglas estrictas:

confidence < 0.85 â†’ NO asumir

Campos ambiguos â†’ null

Nunca inventar servicios, comunas o urgencias fuera de allowlist

Nunca â€œcompletarâ€ datos por intuiciÃ³n

9. Principios productivos (NUEVO Â· CRÃTICO)

A partir de ahora, toda decisiÃ³n debe considerar:

Estabilidad operacional

El sistema debe resistir:

mensajes duplicados

latencia

reintentos de webhook

fallos parciales

Observabilidad

Todo paso relevante debe quedar logueado

Logs deben permitir:

reconstruir una conversaciÃ³n

auditar decisiones

detectar cuellos de botella

Seguridad

Tokens protegidos

Inputs validados

Nada crÃ­tico controlado por prompt

Costos

Uso de OpenAI optimizado

No usar LLM donde una regla basta

10. Forma de trabajo esperada del asistente

Cuando se entregue un ZIP, el asistente debe:

Analizar el ZIP completo

Detectar errores, riesgos y deudas tÃ©cnicas

Proponer solo cambios necesarios

Entregar scripts completos (copy/paste)

Indicar con precisiÃ³n:

archivo

cambio

motivo

Entregar pasos claros para implementar

Nunca asumir contexto fuera del ZIP + este PROMPT.

11. Prioridades tÃ©cnicas (ordenadas)

Estabilidad del sistema

Flujo END-TO-END funcional

Observabilidad

PreparaciÃ³n para producciÃ³n

Escalabilidad futura

OptimizaciÃ³n fina

12. Criterio de calidad esperado

Las soluciones deben ser:

Profesionales

Auditables

DeterminÃ­sticas

Reproducibles

Seguras ante fallos

Aptas para producciÃ³n real

No se aceptan:

hacks rÃ¡pidos

lÃ³gica implÃ­cita en prompts

decisiones â€œporque sÃ­â€

dependencias frÃ¡giles

13. Resultado esperado de cada iteraciÃ³n

Cada iteraciÃ³n debe dejar el proyecto:

compilando

levantando en Docker

con flujo completo funcional

sin romper nada previo

un paso mÃ¡s cerca de producciÃ³n

Fin del PROMPT MAESTRO v2 â€“ CONECTAPRO